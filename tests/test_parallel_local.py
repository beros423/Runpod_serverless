"""
ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ í´ë¼ì´ì–¸íŠ¸ (ë¡œì»¬ Mock ì„œë²„ìš©)
"""
import asyncio
import aiohttp
import time
import random
from typing import List, Dict
import json
from datetime import datetime


class LocalMockProcessor:
    """ë¡œì»¬ Mock ì„œë²„ë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self, base_url: str = "http://localhost:5000", num_workers: int = 5):
        self.base_url = base_url
        self.num_workers = num_workers
        self.endpoint_id = "test-endpoint"
    
    async def submit_job(self, session: aiohttp.ClientSession, input_data: Dict) -> str:
        """ì‘ì—… ì œì¶œ"""
        url = f"{self.base_url}/v2/{self.endpoint_id}/run"
        async with session.post(url, json={"input": input_data}) as response:
            result = await response.json()
            return result.get("id")
    
    async def check_status(self, session: aiohttp.ClientSession, job_id: str) -> Dict:
        """ì‘ì—… ìƒíƒœ í™•ì¸"""
        url = f"{self.base_url}/v2/{self.endpoint_id}/status/{job_id}"
        async with session.get(url) as response:
            return await response.json()
    
    async def wait_for_completion(self, session: aiohttp.ClientSession, job_id: str, 
                                  max_wait: int = 300, poll_interval: float = 0.5) -> Dict:
        """ì‘ì—… ì™„ë£Œ ëŒ€ê¸°"""
        start_time = time.time()
        while time.time() - start_time < max_wait:
            status = await self.check_status(session, job_id)
            
            if status.get("status") == "COMPLETED":
                return status
            elif status.get("status") in ["FAILED", "CANCELLED"]:
                raise Exception(f"Job {job_id} failed: {status}")
            
            await asyncio.sleep(poll_interval)
        
        raise TimeoutError(f"Job {job_id} timed out after {max_wait} seconds")
    
    async def process_single_job(self, session: aiohttp.ClientSession, 
                                 input_data: Dict, job_index: int) -> Dict:
        """ë‹¨ì¼ ì‘ì—… ì²˜ë¦¬"""
        print(f"[Worker {job_index+1:2d}] ì‘ì—… ì œì¶œ ì¤‘...")
        
        submit_time = time.time()
        job_id = await self.submit_job(session, input_data)
        
        print(f"[Worker {job_index+1:2d}] Job ID: {job_id[:8]}... - ëŒ€ê¸° ì¤‘...")
        
        result = await self.wait_for_completion(session, job_id)
        complete_time = time.time()
        
        elapsed = complete_time - submit_time
        wait_time = result.get("output", {}).get("wait_time", 0)
        
        print(f"[Worker {job_index+1:2d}] âœ… ì™„ë£Œ! (ëŒ€ê¸°: {wait_time:.2f}ì´ˆ, ì „ì²´: {elapsed:.2f}ì´ˆ)")
        
        return {
            "job_index": job_index,
            "job_id": job_id,
            "input": input_data,
            "output": result.get("output"),
            "wait_time": wait_time,
            "total_time": elapsed,
            "status": result.get("status")
        }
    
    async def process_batch_parallel(self, input_list: List[Dict]) -> List[Dict]:
        """ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬"""
        async with aiohttp.ClientSession() as session:
            tasks = [
                self.process_single_job(session, input_data, idx)
                for idx, input_data in enumerate(input_list)
            ]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            return results
    
    async def process_batch_sequential(self, input_list: List[Dict]) -> List[Dict]:
        """ë°°ì¹˜ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬"""
        results = []
        async with aiohttp.ClientSession() as session:
            for idx, input_data in enumerate(input_list):
                result = await self.process_single_job(session, input_data, idx)
                results.append(result)
        return results
    
    async def save_results_to_files(self, results: List[Dict], output_dir: str = "results"):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        for result in results:
            if isinstance(result, Exception):
                continue
            
            job_id = result["job_id"]
            result_text = result.get("output", {}).get("result_text", "")
            
            # ê°œë³„ ê²°ê³¼ íŒŒì¼ ì €ì¥
            filename = f"{output_dir}/result_{result['job_index']:02d}_{job_id[:8]}.txt"
            with open(filename, "w", encoding="utf-8") as f:
                f.write(result_text)
            
            print(f"ğŸ’¾ ì €ì¥: {filename}")
        
        # ì „ì²´ ìš”ì•½ ì €ì¥
        summary_file = f"{output_dir}/summary.json"
        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump({
                "timestamp": datetime.now().isoformat(),
                "total_jobs": len(results),
                "successful": len([r for r in results if not isinstance(r, Exception)]),
                "failed": len([r for r in results if isinstance(r, Exception)]),
                "results": [r for r in results if not isinstance(r, Exception)]
            }, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š ìš”ì•½ ì €ì¥: {summary_file}")


async def test_parallel_performance():
    """ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 70)
    print("ğŸ§ª ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 70)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    num_jobs = 10
    test_inputs = [
        {
            "task_name": f"ì‘ì—…_{i+1}",
            "wait_time": round(random.uniform(1, 3), 2),  # 1-3ì´ˆ ëœë¤
            "data": f"í…ŒìŠ¤íŠ¸ ë°ì´í„° {i+1}"
        }
        for i in range(num_jobs)
    ]
    
    total_expected_time = sum(inp["wait_time"] for inp in test_inputs)
    print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ ì„¤ì •:")
    print(f"   - ì‘ì—… ìˆ˜: {num_jobs}ê°œ")
    print(f"   - ëŒ€ê¸° ì‹œê°„ í•©ê³„: {total_expected_time:.2f}ì´ˆ")
    print(f"   - ì˜ˆìƒ ìˆœì°¨ ì²˜ë¦¬ ì‹œê°„: ~{total_expected_time:.1f}ì´ˆ")
    print()
    
    processor = LocalMockProcessor(num_workers=5)
    
    # 1. ë³‘ë ¬ ì²˜ë¦¬
    print("\n" + "-" * 70)
    print("ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ (5ê°œ ë™ì‹œ ì‹¤í–‰)")
    print("-" * 70)
    
    start_parallel = time.time()
    parallel_results = await processor.process_batch_parallel(test_inputs)
    parallel_time = time.time() - start_parallel
    
    print(f"\nâœ… ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: {parallel_time:.2f}ì´ˆ")
    
    # ê²°ê³¼ ì €ì¥
    await processor.save_results_to_files(parallel_results, "results_parallel")
    
    # 2. ìˆœì°¨ ì²˜ë¦¬ (ë¹„êµìš©)
    print("\n" + "-" * 70)
    print("ğŸŒ ìˆœì°¨ ì²˜ë¦¬ (1ê°œì”© ìˆœì°¨ ì‹¤í–‰)")
    print("-" * 70)
    
    start_sequential = time.time()
    sequential_results = await processor.process_batch_sequential(test_inputs)
    sequential_time = time.time() - start_sequential
    
    print(f"\nâœ… ìˆœì°¨ ì²˜ë¦¬ ì™„ë£Œ: {sequential_time:.2f}ì´ˆ")
    
    # ê²°ê³¼ ì €ì¥
    await processor.save_results_to_files(sequential_results, "results_sequential")
    
    # ê²°ê³¼ ë¹„êµ
    print("\n" + "=" * 70)
    print("ğŸ“Š ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
    print("=" * 70)
    
    speedup = sequential_time / parallel_time if parallel_time > 0 else 0
    time_saved = sequential_time - parallel_time
    efficiency = (speedup / 5) * 100  # 5ê°œ ì›Œì»¤ ê¸°ì¤€
    
    print(f"\nìˆœì°¨ ì²˜ë¦¬ ì‹œê°„:     {sequential_time:>8.2f}ì´ˆ")
    print(f"ë³‘ë ¬ ì²˜ë¦¬ ì‹œê°„:     {parallel_time:>8.2f}ì´ˆ")
    print(f"{'â”€' * 40}")
    print(f"ì†ë„ í–¥ìƒ:         {speedup:>8.2f}ë°°")
    print(f"ì ˆì•½ëœ ì‹œê°„:       {time_saved:>8.2f}ì´ˆ ({time_saved/sequential_time*100:.1f}%)")
    print(f"ë³‘ë ¬í™” íš¨ìœ¨:       {efficiency:>8.1f}%")
    
    # ì„±ê³µë¥ 
    parallel_success = len([r for r in parallel_results if not isinstance(r, Exception)])
    sequential_success = len([r for r in sequential_results if not isinstance(r, Exception)])
    
    print(f"\në³‘ë ¬ ì²˜ë¦¬ ì„±ê³µë¥ :   {parallel_success}/{num_jobs} ({parallel_success/num_jobs*100:.0f}%)")
    print(f"ìˆœì°¨ ì²˜ë¦¬ ì„±ê³µë¥ :   {sequential_success}/{num_jobs} ({sequential_success/num_jobs*100:.0f}%)")
    
    print("\n" + "=" * 70)
    print("âœ¨ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 70)
    
    return {
        "sequential_time": sequential_time,
        "parallel_time": parallel_time,
        "speedup": speedup,
        "efficiency": efficiency
    }


async def simple_test():
    """ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ (5ê°œ ì‘ì—…ë§Œ)"""
    print("\n" + "=" * 70)
    print("ğŸ¯ ê°„ë‹¨í•œ ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    
    import random
    
    # 5ê°œ ì‘ì—…
    test_inputs = [
        {"task": f"ì‘ì—…_{i+1}", "wait_time": round(random.uniform(1, 3), 2)}
        for i in range(5)
    ]
    
    processor = LocalMockProcessor(num_workers=5)
    
    print("\n5ê°œ ì‘ì—…ì„ ë™ì‹œì— ì²˜ë¦¬í•©ë‹ˆë‹¤...\n")
    
    start = time.time()
    results = await processor.process_batch_parallel(test_inputs)
    elapsed = time.time() - start
    
    print(f"\nâœ… ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
    
    # ê²°ê³¼ ì €ì¥
    await processor.save_results_to_files(results, "results_simple")
    
    return results


if __name__ == "__main__":
    import random
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          ë¡œì»¬ Mock ì„œë²„ ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ í´ë¼ì´ì–¸íŠ¸              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ì‹¤í–‰ ì „ í™•ì¸:
  1. mock_server.pyê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”
  2. ì„œë²„ ì£¼ì†Œ: http://localhost:5000

í…ŒìŠ¤íŠ¸ ì˜µì…˜:
  1. ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ (5ê°œ ì‘ì—…)
  2. ì „ì²´ ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ (10ê°œ ì‘ì—…, ìˆœì°¨ vs ë³‘ë ¬)
""")
    
    choice = input("ì„ íƒí•˜ì„¸ìš” (1 ë˜ëŠ” 2, ê¸°ë³¸ê°’ 1): ").strip() or "1"
    
    if choice == "2":
        asyncio.run(test_parallel_performance())
    else:
        asyncio.run(simple_test())
